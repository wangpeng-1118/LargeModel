# Introduction

计算机视觉领域处理的图像大小一般为224 * 224，直接将图片的像素数据输入到transformer中的想法不切实际，因为维度太长了。

CVPR18 Local Network：像素点作为transformer的直接输入导致序列太长，我们可以把网络中间的特征图当作transformer的输入。

Stand-Alone Attention（孤立自注意力）：不使用完整的一张图，而是整张图的一部分

Axial Attention（轴注意力）：由于图片是一个2D的数据，我们可以把它看作是两个1D的序列，首先再高度上面做一次自注意力，然后在宽度上面做一次自注意力

以上的注意力机制都比较特殊，没有拌饭在现有的（2020）硬件上进行加速。所以截至2020年，计算机视觉的模型相对于千亿级别的transformer模型还是相差很远

**本论文提出的方法**：将图片分成若干个16 * 16的patch，可以将每个大小为16 * 16的patch当成NLP中的token，将一张图片当成输入给transformer的一个句子。并且使用有监督学习方法

在ImageNet上的数据集上面进行训练时如果不添加比较强的约束，ViT的模型跟同等大小的残差网络相比会可预期地弱一些，因为跟残差网络相比，ViT缺少一些归纳偏置（inductive biases）。

Inductive Biases（先验知识，预先的假设）

对于卷积神经网络，我们常说有两个归纳偏置：

* **locality**：卷积核在图片上进行滑动，这里假设了图片上相邻的区域会有相邻的特征（相近的东西，相关性越强，比较合理）
* **translation equivariance**（平移同变性）：$f(g(x)) = g(f(x))$，无论是先做平移还是先做卷积，只要是同样的输入进来，遇到同样的卷积核，那么它的输出永远是一样的

CNN有这两个先验知识，所以它只需要相对少的数据去学习一个比较好的模型。对于transformer来说，它对视觉世界的感知全部需要自己学习。

实验证明，在更大规模的数据集上面进行预训练，就能在下游任务上面获得很好的迁移效果

# Related Work


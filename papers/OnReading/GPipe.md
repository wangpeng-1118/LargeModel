# Background Knowledge

表示模型的参数的数量时，M表示百万，B表示十亿

**BLEU（Bilingual Evaluation Understudy）** 是一个用于自动评估机器翻译或文本生成质量的指标。它的核心思想是：将模型生成的文本（候选句）与一个或多个参考答案（人工标注的理想输出）进行对比，看有多少 n-gram（连续的词组）匹配。

* 通常会计算 **1-gram、2-gram、3-gram、4-gram** 的匹配程度。

- BLEU 值范围是 **0 到 1**，常以 **0 到 100** 的百分比表示。数值越高，表示模型生成的文本与参考答案越相似。

**average BLEU**（平均 BLEU）指的是：

> **对一个测试集中的所有样本，各自计算 BLEU 分数后，再求一个平均值。**

“模型的不同架构”指的是**神经网络结构的不同设计方式**，它们决定了模型如何处理输入数据、提取特征、生成输出。不同任务（如文本分类、机器翻译、图像识别）可能适合不同的架构。常见的架构包括RNN，CNN，Transformer，BERT，GPT。

硬件限制，包括加速器（GPU或TPU）上的内存限制和通信带宽，迫使用户将更大的模型划分为多个分区，并将不同的分区分配给不同的加速器。

**Lingvo** 是一个模块化的 TensorFlow 框架，用于构建复杂的神经网络模型，特别擅长处理 **序列数据**。

# Introduction

借助GPipe，每个模型都可以被定义为一系列层，连续的层组可以被划分为单元。然后将每个单元放置在单独的加速器上。基于这种划分设置，我们提出了一种新颖的带有批量分割的流水线并行算法。我们首先将一个训练样本的小批量划分为更小的微批量，然后在单元上对每组微批量进行流水线执行。

# The GPipe Library

* 将一个神经网络切为k块，分别放到k个GPU中，降低每个GPU保存的模型的开销
* 将数据按批量切开，提升数据的并行度

图2里面的（b），在时间上相对于一个GPU是没有提升的，唯一的提升点可以看作是GPU的内存提升到了原来的四倍，因为每个GPU只需要存储少部分的权重。在（c）图中，批量$F_0$又被分为了四个微批量，当一个GPU处理完前面的微批量并且将数据交给后面的GPU处理之后，可以立即开始处理下一个微批量，相当于数据并行。

**Performance Optimization**

在GPipe中，神经网络被分为很多个partition。**re-materialization**就是在一个micro-batch前向传播结束之后只保留每个partition的输入和输出，而不保留partition中间的activation，等到进行反向传播时，再对前向过程重新计算一次，从而获得反向传播所需要的activation。

$M$表示微批量的个数，$K$表示神经网络被分的层数，也是研究者拥有的GPU的个数。当$M \geq 4 \times K$时bubble time几乎可以忽略不计。

forward的重新计算会带来约30%的开销，GPU之间的通讯开销比较低，因为我们只需要将自己处理的这个partition的输出传给其他的GPU。

尽量保证每个GPU处理的partition部分的开销相当，否则开销最大的那块会成为整个网络的瓶颈

Table 1中Pipeline-2表示有2个GPU同时工作

Pipeline-1指的是在一个GPU的情况下，但是使用了re-materialization，抛弃了一部分的activation。在后续使用多GPU时，模型的参数也接近线性得增长。在使用8个GPU时，模型得参数大概是单GPU的6倍数。

在CNN中，网络将图像的长宽减少，通道数增加，网络只是保证每一层的计算量相当，但是每一层的内存使用率不一样，导致将网络切分成多个块的时候不均匀，进而导致增加卡的时候，中间有一个GPU的内存需要多一些，成为了整个网络的瓶颈。

在transformer中，每一个transformer层的内存占用都差不多，这样在增加GPU的时候，模型的参数量的增长更加线性。

# Performance Analuses

**AmoebaNet是一种通过进化算法自动搜索得到的深度神经网络结构，代表了神经网络结构设计自动化的重要方向。**

Table 4表示具体每个事物开销的占比，丢弃activation导致反向传播时需要重新forward计算中间值，再一次forward带来的开销大约占总计算开销的30%

# 总结

==table 1里面Total Model Parameter Memory从11.7G到8.8G可能数据有误==

将批量数据差分成更小的微批量，可以提升数据的并行度，使用re-Materialization之后可以用来支持更大的模型，但是时间上需要付出20%的额外开销

灵活性？？？（局限性）：GPipe支持任意一个可以表示成序列的模型

可靠性：使用了同步的梯度下降